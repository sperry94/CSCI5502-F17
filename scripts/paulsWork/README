This directory contains the scripts and Python programs used to access and process the Yelp database and the folders containing the review IDs used to run the classifier and the results from running the classifier as well as intermediate results.

Two folders:
   reviews - folder containing lists of review IDs
   results - folder containing the results from running the Naive Bayesian Classifier
`
The following are the scripts and programs:

   Process.py - A Python base class that loops through a list of review IDs.  

   helloWorldProcessing.py - A program that uses Process.py and demostrates how to use the default operation and then overrides the process method.

   YelpDB.py - A Python class to connect to the database and perform the database queries used in this set of scripts/programs.

   binBusinesses.py - This program derived from Process.py and was used to count the number of occurances of each business in the list of review IDs.

   getAllRestaurantReviews.py - This processes all the reviews in the database and outputs only those of the category 'Restaurants'.

   getBusinessInfo.py - This takes a list of business IDs and associated frequency (generated with binBusinesses.py) and outputs the Name, City, and State also.

   getReviewsBusiness.py - This script takes a business ID and produces a list of all the review IDs for that business.

   getReviewsCity.py - This script generates a list of all the Restaurant reviews for a given city.

   naiveBayesClassifier.py - This program uses the nltk Python library.  It takes a list of review IDs in the form of a file input and trains the classifier with 2/3 of the data and tests with 1/3 of the data.  The training/test sets are generated by stripping out stopwords and generating a list with uniqgram and bigram frequency for each review.  A review is consider positive if it received a score of greater than 3 and negative otherwise.  The program prints the accuracy from the test step and then outputs the 100 most informative features.
      
     Note: I used much of Ekaterina Tcareva's scheme for generating the unigram and bigram feature set.  Her code can be found at: https://github.com/Katy-katy/

   produceResultsHighlyReviewedBusinesses - This is just a bash script to run the classifier on a subset of the more highly reviewed businesses.

   produceReviewsCity - This was just a bash script to run the classifier on the set of city reviews.

   produceReviewsHighlyReviewedBusinesses - This is just a bash script to generated lists of review IDs for the more highly reviewed businesses.



Observations and Flow:

The classifier did much better during testing when it had more data to train. This is especially apparent when running the classifier on the individual restaurants.

The most informative features were nearly always negative.  That is, it would appear that strongly unique negative language nearly always indicates a negative review.  Positive language isn't nearly as indicative of a positive review unless it's something like "strong;ly recommend".  

In our testing:

1) We populated the database and did some initial inspections of the data.  It was determined that there was very little need to clean the data.  We also noticed that restaurant data fell into the categories of "Restaurants" but also "Pizza", "Sushi", "Burgers", etc.  We chose to process only those of the category "Restaurants".
2) Did some analysis of the data with respect to the types of businesses and numbers of reviews in the various cities.
3) A preliminary unigram and bigram analysis was done of the data set  (Bada and Zolbo had this info in our earlier review).
4) This directory contains the Naive Bayesian Classifier with:
 
    o a run against all restaurant reviews in the database and an accuracy of 76%
    o runs against 13 cities and accuracy results generally around 72%
    o runs against 22 of the most highly reviewed businesses with accuracy of ~50% (not very useful).
5) Bada and Zolbo did further analysis of individual restaurants.  This analysis includes unigram/bigram analysis but also a more significant analysis where the terms from the unigram/bigram and most informative features from the Naive Bayesian Classifier were used find context and sentiment.  Find those results in their contributions.
6) I believe we can make some assertions from just the classifier but Bada/Zolbo analysis should pull it together.



